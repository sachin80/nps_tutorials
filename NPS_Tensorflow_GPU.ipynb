{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to teach some of the basics of GPU-enabled Tensorflow. This assumes that you already have a GPU-enabled computer with Nvidia's CUDA and cuDNN software. You will have to install the GPU version of Tensorflow as well. For additional information please see the following:\n",
    "\n",
    "https://www.tensorflow.org/install/gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Devices Supported\n",
    "\n",
    "In TensorFlow, the supported device types are CPU and GPU. They are represented as strings. For example:\n",
    "\n",
    "`\"/cpu:0\"`: The CPU of your machine.\n",
    "\n",
    "`\"/device:GPU:0\"`: The GPU of your machine, if you have one.\n",
    "\n",
    "`\"/device:GPU:1\"`: The second GPU of your machine, etc.\n",
    "\n",
    "If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, `matmul` has both CPU and GPU kernels. On a system with devices `cpu:0` and `gpu:0`, `gpu:0` will be selected to run `matmul`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Placement \n",
    "\n",
    "Whenever you run a computation graph, Tensorflow evaluates a node that is not placed on a device by using a \"simple placer\" ruleset to place it, along with all the other nodes that are not placed yet. The simple placer respects the following rules:\n",
    " - If a node was already placed in a previous run of the graph, it is left on that device. \n",
    " - Else, if the user *pinned* a node to a device, the placer places it on that device. \n",
    " - Else, it defaults to GPU#0, or CPU if there is no GPU. \n",
    " \n",
    "We can find out which devices your operations and tensors are assigned to, create the session with `log_device_placement` configuration option set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a 2 by 3 tensor that is named 'a'\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2,3], name='a')\n",
    "# Create a 3 by 2 tensor that is named 'b'\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3,2], name='b')\n",
    "# Multiply both tensors\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Run the operation\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then please go to your shell (Anaconda prompt in my case) to verify the following log message:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:2d:00.0, compute capability: 6.1\n",
    "2019-01-30 16:44:29.162518: I tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:2d:00.0, compute capability: 6.1\n",
    "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2019-01-30 16:44:29.181547: I tensorflow/core/common_runtime/placer.cc:927] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "a: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2019-01-30 16:44:29.186660: I tensorflow/core/common_runtime/placer.cc:927] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "b: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2019-01-30 16:44:29.191640: I tensorflow/core/common_runtime/placer.cc:927] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "[I 16:45:53.197 NotebookApp] Saving file at /NPS_Tensorflow_GPU.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines starting with \"I\" for Info are the log messages. When we create a session, Tensorflow logs a session to tell us that it has found a GPU card. The the first time we run the graph (in this case initializing variable `a`), the simple placer is run and places each node on the device it was assigned to.  Above we see that the operations and tensors are assigned to the default device GPU:0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Ram\n",
    "\n",
    "Tensorflow maps nearly all of the GPU memory of all the GPUs (subject to `CUDA_VISIBLE_DEVICES`) visible to processes. In some cases you may want to run each process on a different on different GPU cards. The easiest way to do this is to set the `CUDA_VISIBLE_DEVICES` environment variable so that each process sees only the appropiate GPU cards. For example if we have 4 GPU cards, we can start 2 programs, program_1.py and program_2.py, with the first program using the first two GPUs. In our shell we would run: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES = 0, 1 python program_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start the second program using the last tow GPUs. Then in another shell run: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES = 2, 3 python program_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more options are:\n",
    "1. The `allow_growth option`, which attempts to allocate only as much GPU memory based on runtime allocations: it starts out allocating very little memory, and as Sessions get run and more GPU memory is needed, we extend the GPU memory region needed by the TensorFlow process. Note that we do not release memory, since that can lead to even worse memory fragmentation (and so your GPU may run out of memory). To turn this option on, set the option in the ConfigProto by:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The second method is the `per_process_gpu_memory_fraction` option, which determines the fraction of the overall amount of memory that each visible GPU should be allocated. For example, you can tell TensorFlow to only allocate 40% of the total memory of each GPU by:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option is a more determinstic approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning to devices\n",
    "\n",
    "If you don't \"pin\" to a device then the whole graph will be placed on the default device. To pin nodes to a device, you must create a device block using the `device()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Here we are creating a \"device\" block, only a and b will be pinned to \n",
    "# to cpu:0 since that is within the context block\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "\n",
    "# This multiplication node in not pinned on any device and so it will be placed \n",
    "# on the default device. \n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "\n",
    "In previous versions of Tensorflow, we had read in data from a source dataset based on `from_tensor_slices()` or `from_tensor()`. Since Tensorflow 1.4 we can use the Tensorflow Data API for reading in data. It handles most of the complexity that we had to deal with previously (e.g., threads). \n",
    "\n",
    "More information on the Data API can be found here:\n",
    "https://www.tensorflow.org/guide/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "test_csv = open(\"my_test.csv\", \"w\")\n",
    "test_csv.write(\"x1, x2 , target\\n\")\n",
    "test_csv.write(\"1.,, 0\\n\")\n",
    "test_csv.write(\"4., 5. , 1\\n\")\n",
    "test_csv.write(\"7., 8. , 0\\n\")\n",
    "test_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"my_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to tell it how to decode each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_csv_line(line):\n",
    "    x1, x2, y = tf.decode_csv(\n",
    "        line, record_defaults=[[-1.], [-1.], [-1.]])\n",
    "    X = tf.stack([x1, x2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can apply this decoding function to each element in the dataset using `map()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.skip(1).map(decode_csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a one-shot iterator using `make_one_shot_iterator()`. A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization. We are also call its `get_next()` method to get a tensor that represents the next element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = dataset.make_one_shot_iterator()\n",
    "X, y = it.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeatedly evaluate `next_element` to go through the dataset. When there are not more elements, we get an `OutOfRangeError`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1.] 0.0\n",
      "[4. 5.] 1.0\n",
      "[7. 8.] 0.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            X_val, y_val = sess.run([X, y])\n",
    "            print(X_val, y_val)\n",
    "    except tf.errors.OutOfRangeError as ex:\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Tensorflow \n",
    "\n",
    "To run graphs across multiple servers you first need to define a *cluster*. A cluster is composed of one or more Tensorflow servers, called *tasks*, typically spread across several machines. Each task belongs to a *job*. A job is a named group of tasks that typically have a common role such as keeping track of the model parameters where the job is usually called \"ps\" (parameter server) or the job is called \"worker\" since this job performs computations.\n",
    "\n",
    "Every Tensorflow server provides tow services: the *master service* and the *worker service*. The master service allows clients to open sessions and use them to run graphs, It coordinates the computations across tasks and allows the worker service to actually execute the computations on the other task adn get thier results. \n",
    "\n",
    "For more information please go to:\n",
    "https://www.tensorflow.org/deploy/distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a Tensorflow server you must create a **Server** object. Let's first create a **Server** object on the local host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.constant(\"Hello distributed TensorFlow!\")\n",
    "# This creates the Server object called \"server\" on the local host\n",
    "server = tf.train.Server.create_local_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello distributed TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a plain local session, like above, each variable's state is managed by the session itself and as soon as it ends the variables are lost. Also, multiple ses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterspec\n",
    "\n",
    "The cluster specification defines two jobs: \"ps\" and \"worker\" containing 2 tasks and 3 tasks. In this example, machine at 127.0.0.1 will listen in on different ports: 2 ports correspond to the ps job and 3 correspond to the worker job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        \"127.0.0.1:2221\",  # /job:ps/task:0\n",
    "        \"127.0.0.1:2222\",  # /job:ps/task:1\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        \"127.0.0.1:2223\",  # /job:worker/task:0\n",
    "        \"127.0.0.1:2224\",  # /job:worker/task:1\n",
    "        \"127.0.0.1:2225\",  # /job:worker/task:2\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start the tasks we must now pass in the cluster spec (so it can communicate with other servers), it's job name, and a task number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ps0 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=0)\n",
    "task_ps1 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=1)\n",
    "task_worker0 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\n",
    "task_worker1 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=1)\n",
    "task_worker2 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning operations across devices and servers\n",
    "\n",
    "You can use device blocks to pin operations to any device managed by any task by specifying the job name and task index. If you omit the task index it will use the default device for that job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/job:ps\"):\n",
    "    a = tf.Variable(1.0, name=\"a\")\n",
    "\n",
    "with tf.device(\"/job:worker\"):\n",
    "    b = a + 2\n",
    "\n",
    "with tf.device(\"/job:worker/task:1\"):\n",
    "    c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow (the client) uses the *gRPC* protocol (Google Remote Procedure Call) to communicate with the server. It is based on HTTP2 which opens a connection and leaves it open during the whole session,\n",
    "allowing bidirectional communication once open. Data is transmitted using *protocol buffers* a lightweight binary data interchange format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Using grpc the client will open a session on the server located at \n",
    "# 127.0.0.1:2221 and it will evaluate c\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\") as sess:\n",
    "    sess.run(a.initializer)\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern when training a neural network on a distributed setup is to store the model parameters on a set of parameter servers (the \"ps\" job) while the other tasks focus on computation (the \"worker\" job). For models with millions of parameters, it's advisable to shard these parameters across multiple servers so to avoid saturating a single parameter server's network card. If you were to manually pin every variable to a different parameter server then it would be tedious. Instead, Tensorflow provides `replica_device_setter`() function which distributes the variables across all the ps tasks in a round-robin fashion, you must also pass in the number of ps tasks and job names for that device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(tf.train.replica_device_setter(\n",
    "        ps_tasks=2,\n",
    "        ps_device=\"/job:ps\",\n",
    "        worker_device=\"/job:worker\")):\n",
    "    v1 = tf.Variable(1.0, name=\"v1\")  # pinned to /job:ps/task:0 (defaults to /cpu:0)\n",
    "    v2 = tf.Variable(2.0, name=\"v2\")  # pinned to /job:ps/task:1 (defaults to /cpu:0)\n",
    "    v3 = tf.Variable(3.0, name=\"v3\")  # pinned to /job:ps/task:0 (defaults to /cpu:0)\n",
    "    s = v1 + v2            # pinned to /job:worker (defaults to task:0/cpu:0)\n",
    "    with tf.device(\"/task:1\"):\n",
    "        p1 = 2 * s         # pinned to /job:worker/task:1 (defaults to /cpu:0)\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            p2 = 3 * s     # pinned to /job:worker/task:1/cpu:0\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "\n",
    "with tf.Session(\"grpc://127.0.0.1:2221\", config=config) as sess:\n",
    "    v1.initializer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
