{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Working Session - Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aocoe/working/handsonml/env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Usual imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Let's import the Keras library\n",
    "from tensorflow import keras\n",
    "\n",
    "# Let's import the OS to use specific features \n",
    "import os \n",
    "\n",
    "# Plotting imports\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "Today we will get into discussing how to train larger neural networks. First let's go over some common activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Commonly used activation functions for neural networks are \n",
    "\n",
    "**Logistic function (a.k.a. Logit, a.k.a. Sigmoid)**:\n",
    "\n",
    "$f(z) = \\sigma(z)= \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "**Rectified Linear Unit (ReLu)**:\n",
    "\n",
    "$f(z) = \\max(0, z)$\n",
    "\n",
    "**Hyperbolic Tangent (TanH)**:\n",
    "\n",
    "$f(z) = \\tanh(z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} = 2\\sigma(2z) - 1$\n",
    "\n",
    "Let's plot these functions and look at their derivatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic function\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Relu function\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Performs derivative of the function, f, up to the eps\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data to plot\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "# Subplot for activation functions\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2,label=\"logit\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"tanh\")\n",
    "plt.plot(z, relu(z), \"m-\", linewidth=2,label=\"relu\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 2])\n",
    "\n",
    "# Subplot for derivative of activation functions\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign,z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, derivative(logit,z), \"g--\", linewidth=2,label=\"logit\")\n",
    "plt.plot(z, derivative(np.tanh,z), \"b-\", linewidth=2, label=\"tanh\")\n",
    "plt.plot(z, derivative(relu,z), \"m-\", linewidth=2,label=\"relu\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.title(\"Derivative of Activation Functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the above activation functions. \n",
    "\n",
    "For the *step* function we can see that the derivative is 0 and so there is no gradient to work with. The *logistic* function remedies this since it has a nonzero derivative everywhere allowing Gradient Descent to work at every step. \n",
    "\n",
    "The *hyperbolic tangent* function is similar to the logistic function in that it is continuous and differentiable but the output ranges from $-1$ to $1$. This tends to make each layer's output more normalized (centered at zero) and so this helps with speeding up training. \n",
    "\n",
    "The *recitified linear unit* is continuous but not differentiable, as it abruptly changes and this can make Gradient Descent bounce around. It has the advantage of being faster to compute and also the maximum output is unbounded, which can improve training as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Single Perceptron*:\n",
    "\n",
    "Let's first present the artificial neuron called a linear threshold unit (LTU) which is the building block of the Perceptron:\n",
    "\n",
    "<img src = 'LTU.png'>\n",
    "\n",
    "where it computes a weighted sum of its inputs \n",
    "\n",
    "$ z = w_1x_1 + w_2x_2 + \\cdot\\cdot\\cdot + w_nx_n = \\mathbf{w}^{T}\\cdot\\mathbf{x}$.\n",
    "\n",
    "The above figure is also know as a linear threshold unit (or artificial neuron). \n",
    "\n",
    "A Perceptron is composed of a single layer of LTUs connected to all the input neurons and input biases. A perceptron with two input neurons and three outputs is shown below. This can classify instances into 3 different binary classes. \n",
    "\n",
    "<img src = 'perceptron_diagram.png'>\n",
    "\n",
    "The learning rule (weight update) for a Perceptron can be summarized as: \n",
    "\n",
    "$w_{i,j}\\leftarrow w_{i,j} + \\eta (y_j - \\hat{y}_j)x_i$\n",
    "\n",
    "where\n",
    " - $w_{i,j}$ is the connection weight between the $i^{th}$ input neuron and the $j^{th}$ output neuron.   \n",
    " - $x_i$ is the $i^{th}$ input value of the current training instance.   \n",
    " - $\\hat{y}_j$ is the output of the $j^{th}$ ouput neuron for the current training instance. \n",
    " - $y_j$ is the target output of the $j^{th}$ output neuron for the current training instance. \n",
    " - $\\eta$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Multi-layer Perceptrons and Backpropagation*\n",
    "\n",
    "In their paper *Perceptrons*, Seymour Papert and Marvin Minsky highlighted serious weaknesses of the single Perceptron learning rule (unable to solve Exclusive OR (XOR) classfication problem). To eliminate these weaknesses we can instead stack multiple Perceptrons together. This is called a *Multi-layer Perceptron*. An example is shown below\n",
    "\n",
    "<img src = 'multi_layer_perceptron.png'>\n",
    "\n",
    "These artificial neural networks can be composed of one input layer, one or more hidden layers, and one final layer. Every layer except the last layer includes a bias neuron and is fully connected. Note, that a neural network with 2 or more layers is considered a *deep* neural network.\n",
    "\n",
    "The multi-layer Percpetion above is modeled as:\n",
    "\n",
    "$z^{[2]} = XW^{[1]} + b^{[1]}$\n",
    "\n",
    "$a^{[2]} = f(z^{[2]})=h_w(z^{[2]})$\n",
    "\n",
    "$z^{[3]} = a^{[2]}W^{[2]} + b^{[2]} $\n",
    "\n",
    "$\\hat{y} = softmax(z^{[3]})$  \n",
    "\n",
    "*where*:\n",
    "\n",
    "* $X\\in\\mathbb{R}^{m\\times n}$ are *the inputs* where $n$ is the number of instances of data and $n$ is the number of features,\n",
    "* $W^{[1]}\\in\\mathbb{R}^{n\\times h}$ are *the 1st layer weights* where the superscript $[1]$ is used for layer number one and $h$ is the number of hidden units in layer 1,\n",
    "* $b^{[1]}\\in\\mathbb{R}^{n\\times h} $ are *the 1st layer bias term* ,\n",
    "* $z^{[2]}\\in\\mathbb{R}^{n\\times h}$ are *the outputs* from first layer's weights,  \n",
    "* $f(\\cdot) = h_w(\\cdot)$ is *the non-linear activation function*,\n",
    "* $a^{[2]}\\in\\mathbb{R}^{n\\times h}$ are *the outputs* of activation function applied first layer's outputs,\n",
    "* $W^{[2]}\\in\\mathbb{R}^{h\\times k}$ are *the 2nd layer weights* where $k$ is the number of classes,\n",
    "* $b^{[2]}\\in\\mathbb{R}^{h\\times k}$ are *the 2nd layer bias term*, \n",
    "* $\\hat{y}\\in\\mathbb{R}^{m\\times k}$ are *the prediction* probabilities where $m$ is the number of instances or examples\n",
    "* $softmax$ will perform a softmax on the output of the last hidden layer. \n",
    " \n",
    "Note, in the example above we are not performing an activation on the last hidden layer. In general, this is done in practice since we will be losing information if we were to use an activation. Instead we use the outputs of the last layer as scores into the softmax layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Output Layer is Softmax*:\n",
    "\n",
    "Generally, used as classification for many classes. Each class has it's own dedicated weights that are updated with backprop. Given an instance $x^{(i)}$, softmax will compute a score for each class, $k$: \n",
    "\n",
    "$s_{k}(x^{(i)}) = z^{[l]}$\n",
    "\n",
    "where:\n",
    "* $s_{k}$ is the score for each class, $k$. This will give us back a score vector that is $n$ or as long as number of feature (classes)\n",
    "* $ z^{[l]}$ output of the last hidden layer, $[l]$\n",
    "\n",
    "Once we have computed the score of every class for instance $x^{(i)}$ you can estimate the probability $\\hat{p}_{k}$ that the instance belongs to class $k$ by taking the running score through a softmax. The softmax computes the exponential of every score then normalizes it:\n",
    "\n",
    "$\\hat{y}_{k}=\\hat{p}_{k} = \\frac{e^{s_{k}(x^{(i)})}}{\\sum_{j=1}^{n}e^{s_{j}(x^{(i)})}}$ \n",
    "\n",
    "Note: sometimes $\\hat{p}_{k}$ is used to denote the prediction probabilites to clarify that these are probabilites. \n",
    "\n",
    "where:\n",
    "* $n$ is the total of classes \n",
    "* $\\sum_{j=1}^{n}e^{s_{j}(x^{(i)})}$ is the running sum of the vector containing scores of each class for the instance $x^{(i)}$, which will be one score. \n",
    "\n",
    "Let's say the input to our $softmax$ function is $[1.2, 0.9, 0.75]$. The output of the softmax function will be $\\hat{y} = [0.42, 0.31, 0.27]$. So now we can use these as probabilities for the value to be in each class. Note, that is is the same as our prediction \n",
    "\n",
    "The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Summary of Steps*:\n",
    "\n",
    "1. Randomly initialize the model's weights $W$, $b$ (we'll cover more effective initalization strategies).\n",
    "2. Feed inputs $X$ into the model to do the forward pass and receive the probabilities.\n",
    "3. Compare the predictions $\\hat{y}$ (ex.  [0.3, 0.3, 0.4]]) with the actual target values $y$ (ex. class 2 would look like [0, 0, 1]) with the objective (cost) function to determine loss $J$. A common objective function for classification tasks is cross-entropy loss. \n",
    "4. Calculate the gradient of loss $J(W,b)$ w.r.t to the model weights, $W$. \n",
    "5. Apply backpropagation to update the weights using gradient descent. The updates will penalize the probabiltiy for the incorrect classes and encourage a higher probability for the correct class ($y$).\n",
    "6. Repeat steps 2 - 4 until model performs well.\n",
    "\n",
    "***Summary***: \n",
    "\n",
    "* *Objective:*  Predict the probability of class $y$ given the inputs $X$. Non-linearity is introduced to model the complex, non-linear data.\n",
    "* *Advantages:*\n",
    "  * Can model non-linear patterns in the data really well.\n",
    "* *Disadvantages:*\n",
    "  * Overfits easily.\n",
    "  * Computationally intensive as network increases in size.\n",
    "  * Not easily interpretable.\n",
    "* *Miscellaneous:* Future neural network architectures that we'll see use the MLP as a modular unit for feed forward operations (affine transformation (XW) followed by a non-linear operation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use Tensorflow to build a multi-layer Perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data from the Keras library\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Here we are performing a per pixel normalization \n",
    "# for the feature data\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28)/255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1,28*28)/255.0\n",
    "\n",
    "# Label data with the target classes\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# Let's split the training dataset into \n",
    "# vaildation (holdout after test set) and train data\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this notebook stable across platforms, run this\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# MNIST it's 28*28 since there is one feature per pixel\n",
    "n_inputs = 28 * 28 \n",
    "# Number of neurons in hidden layer 1\n",
    "n_hidden1 = 300\n",
    "# Number of neurons in hidden layer 2\n",
    "n_hidden2=100\n",
    "# Number of output label to classify\n",
    "n_outputs = 10\n",
    "\n",
    "# We don't know the size of the training batch at this point\n",
    "# and so the shape of X is (None, n_inputs)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a neuron layer function that takes in \n",
    "parameters that specify: \n",
    "\n",
    "- the feature inputs, \n",
    "- the number of neurons, \n",
    "- activation function, \n",
    "- name of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    # Scope for specific layer\n",
    "    with tf.name_scope(name): \n",
    "        \n",
    "        # Get number of feature inputs, \n",
    "        # it's the second dimension (cols) since first is number of instances (rows)\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        \n",
    "        # See http://cs231n.github.io/neural-networks-2/ for std discussion\n",
    "        # Always want to randomly initialize the weight matrix\n",
    "        # If there are too many symmetries intially then gd will update the wieghts the same way\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        \n",
    "        # Create the weight matrix (also called kernel), contains all weights between each input and each neuron,\n",
    "        # Shape of weight matrix is (n_inputs, n_neurons)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        \n",
    "        # Create biases, initialized to 0 \n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        \n",
    "        # Create the subgraph\n",
    "        # Using broadcasting to add 1D array b to 2D array X*W, \n",
    "        # this will add the 1D bias array to every every row in matrix. \n",
    "        Z = tf.matmul(X, W) + b\n",
    "        \n",
    "        # Activation parameter \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the deep neural net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using named scopes \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    # neuron_layer(input_layer, number of neuron, name, activation)\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", \n",
    "                          activation = tf.nn.relu)\n",
    "    \n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", \n",
    "                          activation = tf.nn.relu)\n",
    "    \n",
    "    # Last layer has no activation function\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the usual MSE we would now like a model that estimates a high probability for a target class. If we minimize the cross-entropy we get this objective since it penalizes the low-probability model. This can be used to measure how well a set of estimated class probabilities match the target class. The objective (loss) to minimize is:\n",
    "\n",
    "$J(W,b) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n=\\text{# of classes}}y_k^{(i)}\\log(\\hat{y}_k^{(i)})$\n",
    "\n",
    "where $y_k^{(i)}$ is equal to 1 if the target class for the $i$th instance is class $k$, otherwise it's equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    # Cross-entropy is calculated based on last layer \"logits\"\n",
    "    # It expects labels in the form 0 to number_of_classes - 1\n",
    "    # Output is a 1D tensor with cross-entropy for each \n",
    "    # instance of data\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    \n",
    "    # This will compute the mean of the cross-entropy across \n",
    "    # all instances\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training op\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the results \n",
    "with tf.name_scope(\"eval\"):\n",
    "    \n",
    "    # Below we are checking if the highest logit corresponds to \n",
    "    # the target class. This returns a 1D tesnor full of bools\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    \n",
    "    # Cast the above bools into floats\n",
    "    # Take the average and this will give us the network's \n",
    "    # overall accuracy.\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global variables, create a Saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of epochs\n",
    "n_epochs = 40\n",
    "# Define mini-batch size\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a new batch function that randomly shuffles the data but will create mini-batches and so the whole dataset will be shuffled through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_mini_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    # The number of mini-batches\n",
    "    n_batches = len(X) // batch_size\n",
    "    # Run through each \n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        # \"yield\" keyword remembers where loop left off\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate our training ('batch') accuracy and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.9 Val accuracy: 0.9146\n",
      "1 Batch accuracy: 0.94 Val accuracy: 0.9322\n",
      "2 Batch accuracy: 0.94 Val accuracy: 0.9426\n",
      "3 Batch accuracy: 0.9 Val accuracy: 0.949\n",
      "4 Batch accuracy: 0.96 Val accuracy: 0.9552\n",
      "5 Batch accuracy: 0.94 Val accuracy: 0.9556\n",
      "6 Batch accuracy: 1.0 Val accuracy: 0.961\n",
      "7 Batch accuracy: 0.94 Val accuracy: 0.9616\n",
      "8 Batch accuracy: 0.98 Val accuracy: 0.9632\n",
      "9 Batch accuracy: 0.96 Val accuracy: 0.964\n",
      "10 Batch accuracy: 0.92 Val accuracy: 0.9664\n",
      "11 Batch accuracy: 0.98 Val accuracy: 0.9678\n",
      "12 Batch accuracy: 0.98 Val accuracy: 0.9672\n",
      "13 Batch accuracy: 0.98 Val accuracy: 0.9698\n",
      "14 Batch accuracy: 1.0 Val accuracy: 0.9714\n",
      "15 Batch accuracy: 0.96 Val accuracy: 0.973\n",
      "16 Batch accuracy: 1.0 Val accuracy: 0.9736\n",
      "17 Batch accuracy: 1.0 Val accuracy: 0.9738\n",
      "18 Batch accuracy: 1.0 Val accuracy: 0.975\n",
      "19 Batch accuracy: 0.98 Val accuracy: 0.9748\n",
      "20 Batch accuracy: 1.0 Val accuracy: 0.9748\n",
      "21 Batch accuracy: 1.0 Val accuracy: 0.9756\n",
      "22 Batch accuracy: 0.98 Val accuracy: 0.9762\n",
      "23 Batch accuracy: 0.98 Val accuracy: 0.9752\n",
      "24 Batch accuracy: 0.98 Val accuracy: 0.9756\n",
      "25 Batch accuracy: 1.0 Val accuracy: 0.9772\n",
      "26 Batch accuracy: 0.98 Val accuracy: 0.9768\n",
      "27 Batch accuracy: 1.0 Val accuracy: 0.9774\n",
      "28 Batch accuracy: 0.94 Val accuracy: 0.976\n",
      "29 Batch accuracy: 1.0 Val accuracy: 0.9764\n",
      "30 Batch accuracy: 1.0 Val accuracy: 0.9772\n",
      "31 Batch accuracy: 0.96 Val accuracy: 0.978\n",
      "32 Batch accuracy: 0.96 Val accuracy: 0.9772\n",
      "33 Batch accuracy: 0.98 Val accuracy: 0.9792\n",
      "34 Batch accuracy: 1.0 Val accuracy: 0.9776\n",
      "35 Batch accuracy: 1.0 Val accuracy: 0.977\n",
      "36 Batch accuracy: 0.98 Val accuracy: 0.9788\n",
      "37 Batch accuracy: 1.0 Val accuracy: 0.9776\n",
      "38 Batch accuracy: 0.98 Val accuracy: 0.9792\n",
      "39 Batch accuracy: 1.0 Val accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_mini_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./model_ann_mnist_final.ckpt\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use our model to make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_ann_mnist_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./model_ann_mnist_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = X_test[:20]\n",
    "    # Let's use our last layer, logits, to eval\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    # To get estimated class prob we use softmax() on logits\n",
    "    # To predict a class use argmax(), which will give you \n",
    "    # back the highest logit value\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "Actual classes:    [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", y_test[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorflow's method library to create layers:\n",
    "\n",
    "Let's use `tf.layer.dense` to create layers instead of our own `neuron_layer()`.\n",
    "\n",
    "Note:\n",
    "- several parameters are renamed: scope becomes name, `activation_fn` becomes activation\n",
    "- more differences later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    y_proba = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.9 Validation accuracy: 0.9024\n",
      "1 Batch accuracy: 0.92 Validation accuracy: 0.9254\n",
      "2 Batch accuracy: 0.94 Validation accuracy: 0.9372\n",
      "3 Batch accuracy: 0.9 Validation accuracy: 0.9416\n",
      "4 Batch accuracy: 0.94 Validation accuracy: 0.9472\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9512\n",
      "6 Batch accuracy: 1.0 Validation accuracy: 0.9548\n",
      "7 Batch accuracy: 0.94 Validation accuracy: 0.961\n",
      "8 Batch accuracy: 0.96 Validation accuracy: 0.9622\n",
      "9 Batch accuracy: 0.94 Validation accuracy: 0.9648\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9656\n",
      "11 Batch accuracy: 0.98 Validation accuracy: 0.9666\n",
      "12 Batch accuracy: 0.98 Validation accuracy: 0.9684\n",
      "13 Batch accuracy: 0.98 Validation accuracy: 0.9704\n",
      "14 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.9718\n",
      "16 Batch accuracy: 0.98 Validation accuracy: 0.9726\n",
      "17 Batch accuracy: 1.0 Validation accuracy: 0.9728\n",
      "18 Batch accuracy: 0.98 Validation accuracy: 0.9744\n",
      "19 Batch accuracy: 0.98 Validation accuracy: 0.9758\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9748\n",
      "21 Batch accuracy: 1.0 Validation accuracy: 0.9734\n",
      "22 Batch accuracy: 0.96 Validation accuracy: 0.9752\n",
      "23 Batch accuracy: 0.98 Validation accuracy: 0.9764\n",
      "24 Batch accuracy: 0.98 Validation accuracy: 0.976\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
      "26 Batch accuracy: 0.92 Validation accuracy: 0.9768\n",
      "27 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "28 Batch accuracy: 0.94 Validation accuracy: 0.978\n",
      "29 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9776\n",
      "31 Batch accuracy: 1.0 Validation accuracy: 0.978\n",
      "32 Batch accuracy: 0.96 Validation accuracy: 0.9778\n",
      "33 Batch accuracy: 0.98 Validation accuracy: 0.9784\n",
      "34 Batch accuracy: 0.98 Validation accuracy: 0.9784\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9784\n",
      "36 Batch accuracy: 1.0 Validation accuracy: 0.9792\n",
      "37 Batch accuracy: 1.0 Validation accuracy: 0.9786\n",
      "38 Batch accuracy: 0.98 Validation accuracy: 0.9798\n",
      "39 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "n_batches = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_mini_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./model_ann_mnist_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy improves a little more and the construction phase is much shorter by using  `tf.layer.dense`. Much of the work in these notebooks will use the Tensorflow library instead of our own methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Deep Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients:\n",
    "\n",
    "During Gradient Descent the gradients to update each parameter get smaller and smaller as the updates progress to lower layers where the parameters are virtually unchanged. The training does not converge to a good solution. \n",
    "\n",
    "The reason is that the activation function and the weight initialization technique uses a standard normal distribution. The variance becomes much larger in the later layers causing the the activation function to saturate at the higher layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution 1:*** Use a different initialization technique. One that makes the variance of the output layers close to input layers. \n",
    "\n",
    "We can do this by initializing the connection weights to be random, where $n_{input}$ and $n_{output}$ are the number of input and output connections whose weights are being initialized for the particular layer. \n",
    "\n",
    "*He initialization* \n",
    "\n",
    "This is a popular initialization technique that makes the standard normal distribution with $\\mu=0$ and standard\n",
    "deviation $\\sigma = \\sqrt\\frac{2}{n_{input}+n_{output}}$. It can also be initialized using a uniform distribution between $-r$ and $r$ with $r = \\sqrt\\frac{6}{n_{input}+n_{output}}$. \n",
    "\n",
    "Note, in literature sometimes the initialization variables are shows as $\\text{fan}_{in}$ for $n_{input}$ and $\\text{fan}_{out}$ for $n_{output}$.\n",
    "\n",
    "In Tensorflow, the default variance initializer is the He initialization, which can be called using `tf.variance_scaling_initializer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual setup for MNIST\n",
    "reset_graph()\n",
    "n_input = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the default for the following function is the He initialization\n",
    "# It considers only fan-in (n_inputs)\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                         kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution 2:*** Use non-saturating activation functions.\n",
    "\n",
    "*Leaky-ReLu*\n",
    "\n",
    "Leaky ReLUs allow a small, positive gradient when the unit is not active.\n",
    "\n",
    "This activation function is defined as\n",
    "\n",
    "$f(z) = max(\\alpha z, z)$ \n",
    "\n",
    "Here the hyperparameter $\\alpha$ defines how much the function leaks. It's the slope of the function for $z<0$\n",
    "and is typically set to 0.01. \n",
    "\n",
    "We can use the Leaky-ReLu in Tensorflow by setting `activation = tf.nn.leaky_relu()`.\n",
    "\n",
    "First, let's plot the Leaky-ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLU\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Leaky-ReLu\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"Leaky ReLU activation function\")\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exponential Linear Unit (ELU)*\n",
    "\n",
    "Exponential linear units try to make the mean activations closer to zero which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. The ELUs are defined by:\n",
    "\n",
    "$\\begin{equation}\n",
    "  f(z)=\\begin{cases}\n",
    "    \\alpha(e^{z}-1), & \\text{if $z<0$},\\\\\n",
    "    z, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}$ \n",
    "\n",
    "In Tensorflow, we can use ELUs by setting the activation function to `activation = tf.nn.elu()`\n",
    "\n",
    "Let's plot the ELUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where( z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a Leaky-ReLu in Tensorflow for our MNIST data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tensorflow functions to define leaky-relu function\n",
    "def leaky_relu_tf(z, name=None):\n",
    "    return tf.maximum(0.01*z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu_tf, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu_tf, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28)/255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28)/255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy 0.94 Validation accuracy: 0.9496\n",
      "10 Batch accuracy 0.92 Validation accuracy: 0.9654\n",
      "15 Batch accuracy 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy 1.0 Validation accuracy: 0.9764\n",
      "25 Batch accuracy 1.0 Validation accuracy: 0.9778\n",
      "30 Batch accuracy 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy 1.0 Validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y:y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.2851015593374667&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dnn/hidden1/mul/x&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;dnn/hidden1/mul&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\000d\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.12247448414564133\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 24\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 100\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 100\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Maximum&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/mul/x&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dnn/hidden2/mul/x&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;dnn/hidden2/mul&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;d\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.23354968428611755\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.23354968428611755\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 43\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 100\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Maximum&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;train/gradients/Shape&quot;\\n  input: &quot;train/gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/Fill&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;train/gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;train/gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Maximum&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden2/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/zeros/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/zeros&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape_2&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/zeros/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/GreaterEqual&quot;\\n  op: &quot;GreaterEqual&quot;\\n  input: &quot;dnn/hidden2/mul&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/GreaterEqual&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/GreaterEqual&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/zeros&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Select&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Select_1&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Sum_1&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/Maximum_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/Maximum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/Maximum_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/Maximum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Shape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Mul&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dnn/hidden2/mul/x&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Mul_1&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Sum_1&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/mul_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/Reshape_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/AddN&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;train/gradients/dnn/hidden2/Maximum_grad/tuple/control_dependency_1&quot;\\n  input: &quot;train/gradients/dnn/hidden2/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/AddN&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/AddN&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/AddN&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Maximum&quot;\\n  input: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden1/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape_2&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/zeros/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/zeros&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape_2&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/zeros/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/GreaterEqual&quot;\\n  op: &quot;GreaterEqual&quot;\\n  input: &quot;dnn/hidden1/mul&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Select&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/GreaterEqual&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Select_1&quot;\\n  op: &quot;Select&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/GreaterEqual&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/zeros&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Select&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Select_1&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Sum_1&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/Maximum_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/Maximum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/Maximum_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/Maximum_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Shape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Mul&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;dnn/hidden1/mul/x&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Mul_1&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Sum_1&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/mul_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/mul_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Reshape&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/mul_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/mul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/Reshape_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/mul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/mul_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/AddN_1&quot;\\n  op: &quot;AddN&quot;\\n  input: &quot;train/gradients/dnn/hidden1/Maximum_grad/tuple/control_dependency_1&quot;\\n  input: &quot;train/gradients/dnn/hidden1/mul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;N&quot;\\n    value {\\n      i: 2\\n    }\\n  }\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/AddN_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/AddN_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/AddN_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/Maximum_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval_1/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval_1/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval_1/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval_1/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval_1/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval_1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval_1/Mean&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval_1/Cast&quot;\\n  input: &quot;eval_1/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.2851015593374667&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see the graph\n",
    "from tensorflow_graph_in_jupyter import show_graph\n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization:\n",
    "\n",
    "Although the He initialization and ELU can reduce the vanishing gradient problem at the beginning of training, it does not guarantee that it won't come back. \n",
    "\n",
    "*Internal covariate shift*: problem of the distribution of each layer's inputs change during training as the parameters of the previous layers change. \n",
    "\n",
    "Simply, it zero-centers and normalized the inputs before the activation layer. Therefore, it lets the model learn an optimal scale and mean of inputs for each layer. \n",
    "\n",
    "It does this by analyzing the mean and std of the inputs over the current mini-batch. \n",
    "\n",
    "Algorithm:\n",
    "1. $\\mu_{B}=\\frac{1}{m_{B}}\\sum_{i=1}^{m_B}\\mathbf{x}^{(i)}$\n",
    "2. $\\sigma_{B}^2 = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(\\mathbf{x}^{(i)} - \\mu_{B})^2$\n",
    "3. $\\hat{\\mathbf{x}}^{(i)} = \\frac{(\\mathbf{x}^{(i)} - \\mu_{B})}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}$\n",
    "4. $\\mathbf{z}^{(i)} = \\gamma\\mathbf{x}^{(i)} + \\beta$\n",
    "\n",
    "where\n",
    " -  $\\mu_{B}$ is the empirical mean evaluated over the whole mini-batch $B$. \n",
    " -  $\\sigma_{B}$ is the emprirical standard deviation eval over whole mini-batch\n",
    " - $m_{B}$ is the number of instances in the mini-batch. \n",
    " - $\\mathbf{x}^{(i)}$ is the zero-centered and normalized input\n",
    " - $\\gamma$ is the scaling parameter for the layer. \n",
    " - $\\beta$ is the shifting parameter or offset for the layer. \n",
    " - $\\epsilon$ is a very tiny term usually $10^{-5}$ to avoid a divide by zero. This called the smoothing term. \n",
    " - $\\mathbf{z}^{(i)}$ is scaled and shifted versioin of the inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are 4 parameters to learn for each batch-normalized layer: $\\gamma$ (scale), $\\beta$ (offset), $\\mu$ (mean), $\\sigma$ (std). \n",
    "\n",
    "After estimating, this will heavily reduce the vanishing/exploding gradients problem and also introduces some regularization. \n",
    "\n",
    "*Disadvantages*: it adds some complexity to the model, the neural network makes slower predictions due to extra computations required for each layer. If you want something fast you may want to use something like ELU+He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name = \"X\")\n",
    "\n",
    "# This is set to True during training otherwise it's False.\n",
    "# When True this tells tf.layers.batch_normalization() \n",
    "# to use current mini-batch mean and std (during training) or \n",
    "# use the whole training set's mean and std (during testing).\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# This creates the fully connected layers like we did before.\n",
    "# Note: we are not specifying a per layer activation function \n",
    "# because we want to specify activation after the batch norm \n",
    "# layer.\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "\n",
    "# Now we're creating the batch norm layer, setting it's training and \n",
    "# momentum parameter for batch norm.\n",
    "# Note: given a new value of v, the running average v_avg \n",
    "# is updated: v_avg * momentum + v * (1- momentum)\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is very repetitive and so let's use the *partial()* function as part of the functools in the Python standard library. It allows you to create a wrapper and define default parameters for your function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a Neural Network using Batch Norm, He, and ELU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial #import partial from functools\n",
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    # Here is the wrapper with the parameters that are repeated\n",
    "    # for the batch norm layer\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "    \n",
    "    # Here is the wrapper with the parameters that are repeated\n",
    "    # for the dense layer\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the training phase we need to set the training placeholder to True. Second, the batch_normalization() function creates a few operations that must be evaluated at training step in order to update the moving averages (for the specific training set). \n",
    "\n",
    "We use the UPDATE_OPS collection so we just need to get a list of operations and run/update them at each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8952\n",
      "1 Validation accuracy: 0.9202\n",
      "2 Validation accuracy: 0.9318\n",
      "3 Validation accuracy: 0.9422\n",
      "4 Validation accuracy: 0.9468\n",
      "5 Validation accuracy: 0.954\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.96\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9638\n",
      "10 Validation accuracy: 0.9662\n",
      "11 Validation accuracy: 0.9682\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9726\n",
      "18 Validation accuracy: 0.9738\n",
      "19 Validation accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# Want to get these and update as for each training step\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_mlp_bn_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above accuracy is not great for MNIST. Batch Norm and ELUs shine for much deeper networks. Also, the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. Do not forget these moving averages non-trainable variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Python list comprehension \n",
    "# To find the list of trainable variables\n",
    "[var.name for var in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Python list comprehension \n",
    "# To find the list of global variables\n",
    "[var.name for var in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the lower layers:\n",
    "\n",
    "It's likely that the lower layers have detected low-level features that will be used in image classification across tasks. It's a good idea to freeze the weights. If lower-layer is frozen then it becomes easier to train the higher levels because they won't have to learn a moving target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 100  # reused\n",
    "n_hidden3 = 50  # new!\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the training scope, specifiacally the minimize \n",
    "# function. \n",
    "with tf.name_scope(\"train\"):                                         \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)    \n",
    "    # Use collection TRAINABLE_VARIABLES with scope as parameter\n",
    "    # Using regex where \"hidden[34]|outputs\" means match \n",
    "    # \"hidden\" with possible chars \"3,4\", if not matched\n",
    "    # then match to \"outputs\"\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_mlp_bn_final.ckpt\n",
      "0 Validation accuracy: 0.2282\n",
      "1 Validation accuracy: 0.3176\n",
      "2 Validation accuracy: 0.4288\n",
      "3 Validation accuracy: 0.535\n",
      "4 Validation accuracy: 0.6384\n",
      "5 Validation accuracy: 0.7004\n",
      "6 Validation accuracy: 0.7376\n",
      "7 Validation accuracy: 0.7712\n",
      "8 Validation accuracy: 0.795\n",
      "9 Validation accuracy: 0.814\n",
      "10 Validation accuracy: 0.832\n",
      "11 Validation accuracy: 0.845\n",
      "12 Validation accuracy: 0.8524\n",
      "13 Validation accuracy: 0.8614\n",
      "14 Validation accuracy: 0.8676\n",
      "15 Validation accuracy: 0.872\n",
      "16 Validation accuracy: 0.8768\n",
      "17 Validation accuracy: 0.8804\n",
      "18 Validation accuracy: 0.8848\n",
      "19 Validation accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "# Use collection GLOBAL_VARIABLES\n",
    "# Reuse these variables \n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[12]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_mlp_bn_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_mlp_bn_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to add `stop_gradient()` and all the layer below this addition will be frozen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching frozen layers:\n",
    "\n",
    "Since the frozen layers won't change it's possible to cache the output of the top frozen layer for each training instance. Training goes through the whole dataset many times, so this will give you a huge speed boost as you need to go through the frozen layers once per training instance(instead of once per epoch). Run through all the whole training set through the lower layers, then during training, instead of building training instances you would build batches of outputs from hidden layer 2 and feed them to the training op. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "hidden_1 = 300\n",
    "hidden_2 = 100 \n",
    "hidden_3 = 50\n",
    "hidden_4 = 20\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                              scope=\"hidden[12]\") #uses regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) #restores layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_mlp_bn_final.ckpt\n",
      "0 Validation accuracy: 0.2664\n",
      "1 Validation accuracy: 0.4396\n",
      "2 Validation accuracy: 0.5434\n",
      "3 Validation accuracy: 0.613\n",
      "4 Validation accuracy: 0.655\n",
      "5 Validation accuracy: 0.6934\n",
      "6 Validation accuracy: 0.7336\n",
      "7 Validation accuracy: 0.7644\n",
      "8 Validation accuracy: 0.79\n",
      "9 Validation accuracy: 0.8078\n",
      "10 Validation accuracy: 0.826\n",
      "11 Validation accuracy: 0.839\n",
      "12 Validation accuracy: 0.8466\n",
      "13 Validation accuracy: 0.858\n",
      "14 Validation accuracy: 0.8664\n",
      "15 Validation accuracy: 0.8718\n",
      "16 Validation accuracy: 0.875\n",
      "17 Validation accuracy: 0.8798\n",
      "18 Validation accuracy: 0.8854\n",
      "19 Validation accuracy: 0.8884\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_mlp_bn_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)    \n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "    \n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y:y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the loop for training runs the operation defined earlier (doesn't touch layer 1 and layer 2) and feeds it a a batch of outputs from the second hidden layer (as well as the targets). Tensorflow does not eval any nodes that layer 2 depends on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Zoos:\n",
    "These are pre-trained models released to the public. In a future lecture, we will see this with the Tensorflow Object Detection library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers:\n",
    "\n",
    "So far we have seen four ways to speed up training and reach a better solution:\n",
    "1.  apply good initialization for the connection weights\n",
    "2.  use a good activation function\n",
    "3.  use batch normalization\n",
    "4.  reusing parts of a pretrained network\n",
    "\n",
    "\n",
    "Another way is to use faster optimizers. Here's a list of optimizers that are provided by Tensorflow:\n",
    "\n",
    "- Momentum algorithm\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam Optimization\n",
    "\n",
    "As of late, although some of the adaptive methods are faster, they perform worse on certain types of datasets. \n",
    "\n",
    "It's advisable to stick to Momentum or Nesterov "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling:\n",
    "\n",
    "Finding a learning rate, $\\eta$, can be tricky. Too high can lead to divergence, too low can be too slow. We can use a schedule initially setting it high and then reduce once it stops making fast progress. \n",
    "\n",
    "Here are different types of learning rate schedules:\n",
    "\n",
    " - *Predetermined piecewise constant learning rate*:\n",
    " Set the learning rate to $\\eta_0 = 0.1$ at first, then to $\\eta_1=0.001$ after 50 epochs. Although this can work well it requires lots of fiddling around to figure out the timing.\n",
    " \n",
    " - *Performance scheduling*:\n",
    " Measure the validation error every $N$ steps (just like early stopping) and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping. \n",
    " \n",
    " - *Exponential scheduling*:\n",
    " Set the learning rate to a function of the iteration number $t$: $\\eta(t) = \\eta_0 10^{-\\frac{t}{r}}$, which requires tuning $\\eta_0$ $r$. The learniing rate willl drop by a factor of 10 every iteration step.\n",
    " \n",
    " - *Power scheduling*:\n",
    " Set the learning rate to $\\eta(t) = \\eta_0(1+\\frac{t}{r})^{-c}$ where usually $c=1$. This is similar to exponential scheduling but learning rate drops much more slowly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):    \n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000 # r \n",
    "    decay_rate = 1/10 # eta_0\n",
    "    # Non-trainable var initialized to 0 to keep track of \n",
    "    # current iteration number\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.959\n",
      "1 Validation accuracy: 0.9688\n",
      "2 Validation accuracy: 0.9726\n",
      "3 Validation accuracy: 0.9804\n",
      "4 Validation accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization:\n",
    "\n",
    "To generalize the model better (reduce overfitting) we can use many different types of regularization. Note, the regularization tends to increase the estimator bias while reducing the variance. \n",
    "\n",
    "*L1 regularization*:\n",
    "\n",
    "This type of regularization penalizes the L1 norm of the weights. Also, called ridge regression or Tikunov regularizaton. This will promote the model with parameters closer to 0 (\"simpler\").  \n",
    "\n",
    "The new cost function is now\n",
    "${J}(W)_{new} = J(W)+\\left\\| W\\right\\|_{1}$.\n",
    "\n",
    "Note $\\left\\|W\\right\\|_{1}=\\sum_{ij}|W_{ij}|.$\n",
    "\n",
    "This regularization will promote the weights to have small norm but since the subgradient is $sign(W)$ the gradients are the same regardless of the size of $W$. This will result in sparse solution where $W_{ij}=0$ for several $i$, $j$'s. This may be used for feature selection where features corresponding to zero weights may be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*L2 regularization*:\n",
    "\n",
    " Also, called ridge regression or Tikunov regularizaton. This will promote the model with parameters closer to 0 (\"simpler\"). This regularization will promote the weights to have small norm.\n",
    "\n",
    "The new cost function is now\n",
    "${J}(W)_{new} = J(W)+\\frac{\\alpha}{2}W^{T}W$.\n",
    "\n",
    "The corresponding gradient is \n",
    "$\\nabla_{W}J(W)_{new}= \\alpha W+\\nabla_{W} J(W)$.\n",
    "\n",
    "The update equation is \n",
    "${W}\\leftarrow (1-\\epsilon\\alpha){W} - \\epsilon\\nabla_{W}J(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement L1 regularization manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first implement L! reg manually\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a handle on a tensor using `get_default_graph().get_tensor_by_name()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a handle on the layer weights and compute the total loss.\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 #l1 regularization hyperparameter\n",
    "\n",
    "# Let's now compute the total loss, sum of usual cross entropy loss and l1 loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                             logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_entropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is the same as usual\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9066\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is the pass the regularization function to\n",
    "`tf.layers.dense()`, which will create operations that compute the regularization loss and add these ops to collection of the regularization loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the partial() function so we don't repeat same \n",
    "# arguments.\n",
    "# Note we are also setting the kernel regularizer argument\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu, \n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the regularization loss to the base loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=y, logits=logits)\n",
    "    base_loss=tf.reduce_mean(xentropy, name=\"avg_entropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.908\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9126\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n",
      "10 Validation accuracy: 0.92\n",
      "11 Validation accuracy: 0.9224\n",
      "12 Validation accuracy: 0.9212\n",
      "13 Validation accuracy: 0.9228\n",
      "14 Validation accuracy: 0.9224\n",
      "15 Validation accuracy: 0.9216\n",
      "16 Validation accuracy: 0.9218\n",
      "17 Validation accuracy: 0.9228\n",
      "18 Validation accuracy: 0.9216\n",
      "19 Validation accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout:\n",
    "\n",
    "Dropout is the *most* popular regularization technique for deep NN's. At each training step, evey neuron has a probabilty $p$ of being temporarily dropped, meaning ignored during the training step but it may be active during the next step. The hyperparameter $p$ is called the dropout rate. Typically, this is set to $50\\%$. \n",
    "\n",
    "One way to understand this is that a unique neural network is generated at each training step. Since each neuron can be present or absent, there is a total of $2^{N}$ possible networks ($N$ is the number of dropable neurons). At each of the 10,000 training steps you have essentially trained 10,000 different neural networks each with one training instance. There are not independent networks since they share weights. This can be seen as an averaging ensemble of smaller networks.\n",
    "\n",
    "*Technical note about dropout*:\n",
    "\n",
    "Suppose $p=50\\%$ so that during testing a neuron will be connected to twice as many input neurons (on average) as it was during training. To compensate for this we need to multipy each input neuron's connection weight by $0.5$ after training. If not each neuron will get roughly twice as large as a signal as what the network was trained on. Generally, we need to multiply by the keep probability $(1-p)$ after training or divide each neuron's output by keep prob during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply dropout to the input layer\n",
    "# We can apply to any of the outputs of the hidden layers\n",
    "\n",
    "# Note: we must set training to True when training \n",
    "# and False during test (like Batch Norm)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # = 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9264\n",
      "1 Validation accuracy: 0.9446\n",
      "2 Validation accuracy: 0.9488\n",
      "3 Validation accuracy: 0.9556\n",
      "4 Validation accuracy: 0.9612\n",
      "5 Validation accuracy: 0.9598\n",
      "6 Validation accuracy: 0.9616\n",
      "7 Validation accuracy: 0.9674\n",
      "8 Validation accuracy: 0.967\n",
      "9 Validation accuracy: 0.9706\n",
      "10 Validation accuracy: 0.9674\n",
      "11 Validation accuracy: 0.9678\n",
      "12 Validation accuracy: 0.9698\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.971\n",
      "15 Validation accuracy: 0.9702\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9716\n",
      "18 Validation accuracy: 0.9734\n",
      "19 Validation accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping:\n",
    "\n",
    "Is essentially another way to regularize. Here we stop training as soon as the validation error reaches a minimum. The prediction error on the training set will naturally go down but as does the validation error but at a certain point in th training the validation error stops decreasing and starts to go back up. This is essentially is overfitting the model and so we stop the training early. \n",
    "\n",
    "One way to implement this in Tensorflow is to evaluate the model on a validation set at regular intervals (say 50 steps) and save the \"best\" set if it outperforms the last \"best\" one. We then have a counter that tells us the number of steps since the last \"best\" set was saved and interrupt if it reaches some limit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x11ffc69b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "        training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "        X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "        \n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    dense_layer = partial(tf.layers.dense, \n",
    "                         kernel_initializer=he_init,\n",
    "                         activation=tf.nn.elu)\n",
    "    \n",
    "    hidden1 = dense_layer(X_drop, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = dense_layer(hidden2, n_outputs, activation=None, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        loss = tf.reduce_mean(xentropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        \n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train accuracy: 0.98 Validation accuracy: 0.956 Best valid accuracy: 0.9275\n",
      "Epoch: 1 Train accuracy: 0.96 Validation accuracy: 0.9663333 Best valid accuracy: 0.953\n",
      "Epoch: 2 Train accuracy: 0.94 Validation accuracy: 0.97333336 Best valid accuracy: 0.963\n",
      "Epoch: 3 Train accuracy: 0.92 Validation accuracy: 0.97 Best valid accuracy: 0.97\n",
      "Epoch: 4 Train accuracy: 0.96 Validation accuracy: 0.97366667 Best valid accuracy: 0.97\n",
      "Epoch: 5 Train accuracy: 0.92 Validation accuracy: 0.97833335 Best valid accuracy: 0.9725\n",
      "Epoch: 6 Train accuracy: 0.98 Validation accuracy: 0.9826667 Best valid accuracy: 0.975\n",
      "Epoch: 7 Train accuracy: 0.98 Validation accuracy: 0.981 Best valid accuracy: 0.9755\n",
      "Epoch: 8 Train accuracy: 0.98 Validation accuracy: 0.9816667 Best valid accuracy: 0.976\n",
      "Epoch: 9 Train accuracy: 0.98 Validation accuracy: 0.981 Best valid accuracy: 0.9765\n",
      "Epoch: 10 Train accuracy: 0.98 Validation accuracy: 0.9826667 Best valid accuracy: 0.9765\n",
      "Epoch: 11 Train accuracy: 1.0 Validation accuracy: 0.98333335 Best valid accuracy: 0.979\n",
      "Epoch: 12 Train accuracy: 0.96 Validation accuracy: 0.981 Best valid accuracy: 0.9805\n",
      "Epoch: 13 Train accuracy: 0.98 Validation accuracy: 0.98233336 Best valid accuracy: 0.9805\n",
      "Epoch: 14 Train accuracy: 1.0 Validation accuracy: 0.982 Best valid accuracy: 0.9805\n",
      "Epoch: 15 Train accuracy: 1.0 Validation accuracy: 0.98466665 Best valid accuracy: 0.9805\n",
      "Epoch: 16 Train accuracy: 0.98 Validation accuracy: 0.986 Best valid accuracy: 0.9805\n",
      "Epoch: 17 Train accuracy: 0.98 Validation accuracy: 0.98366666 Best valid accuracy: 0.9805\n",
      "Epoch: 18 Train accuracy: 1.0 Validation accuracy: 0.985 Best valid accuracy: 0.9805\n",
      "Epoch: 19 Train accuracy: 1.0 Validation accuracy: 0.98766667 Best valid accuracy: 0.9805\n",
      "Epoch: 20 Train accuracy: 1.0 Validation accuracy: 0.9853333 Best valid accuracy: 0.981\n",
      "Epoch: 21 Train accuracy: 1.0 Validation accuracy: 0.98366666 Best valid accuracy: 0.981\n",
      "Epoch: 22 Train accuracy: 0.98 Validation accuracy: 0.9863333 Best valid accuracy: 0.981\n",
      "Epoch: 23 Train accuracy: 1.0 Validation accuracy: 0.98733336 Best valid accuracy: 0.981\n",
      "Epoch: 24 Train accuracy: 1.0 Validation accuracy: 0.9856667 Best valid accuracy: 0.983\n",
      "Epoch: 25 Train accuracy: 1.0 Validation accuracy: 0.98833334 Best valid accuracy: 0.983\n",
      "Epoch: 26 Train accuracy: 0.96 Validation accuracy: 0.9856667 Best valid accuracy: 0.983\n",
      "Epoch: 27 Train accuracy: 1.0 Validation accuracy: 0.98333335 Best valid accuracy: 0.983\n",
      "Epoch: 28 Train accuracy: 1.0 Validation accuracy: 0.98466665 Best valid accuracy: 0.983\n",
      "Epoch: 29 Train accuracy: 1.0 Validation accuracy: 0.98333335 Best valid accuracy: 0.983\n",
      "Epoch: 30 Train accuracy: 0.94 Validation accuracy: 0.98466665 Best valid accuracy: 0.9835\n",
      "Epoch: 31 Train accuracy: 1.0 Validation accuracy: 0.9866667 Best valid accuracy: 0.9835\n",
      "Epoch: 32 Train accuracy: 0.98 Validation accuracy: 0.985 Best valid accuracy: 0.9835\n",
      "Epoch: 33 Train accuracy: 1.0 Validation accuracy: 0.986 Best valid accuracy: 0.9835\n",
      "Epoch: 34 Train accuracy: 1.0 Validation accuracy: 0.987 Best valid accuracy: 0.9835\n",
      "Epoch: 35 Train accuracy: 1.0 Validation accuracy: 0.986 Best valid accuracy: 0.9835\n",
      "Epoch: 36 Train accuracy: 1.0 Validation accuracy: 0.986 Best valid accuracy: 0.9835\n",
      "Epoch: 37 Train accuracy: 0.98 Validation accuracy: 0.98433334 Best valid accuracy: 0.9835\n",
      "Epoch: 38 Train accuracy: 1.0 Validation accuracy: 0.9853333 Best valid accuracy: 0.9835\n",
      "Epoch: 39 Train accuracy: 1.0 Validation accuracy: 0.98233336 Best valid accuracy: 0.9835\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./my_best_model_so_far\n",
      "Final accuracy 0.9812\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "best_acc_val = 0\n",
    "check_interval = 100\n",
    "checks_since_last_improvement = 0\n",
    "max_checks_without_improvement = 100\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                acc_val = accuracy.eval(feed_dict={X:mnist.validation.images[:2000], y:mnist.validation.labels[:2000]})\n",
    "                if acc_val > best_acc_val:\n",
    "                    best_acc_val = acc_val\n",
    "                    # This counter will reset every time we improve\n",
    "                    checks_since_last_improvement = 0\n",
    "                    saver.save(sess, \"./my_best_model_so_far\")\n",
    "                else:\n",
    "                    checks_since_last_improvement += 1\n",
    "        # Let's get the training, validation accuracy for the epoch\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X:mnist.validation.images[2000:], y:mnist.validation.labels[2000:]})\n",
    "        \n",
    "        print(\"Epoch:\", epoch, \"Train accuracy:\", acc_train, \n",
    "              \"Validation accuracy:\", acc_val, \n",
    "              \"Best valid accuracy:\", best_acc_val)\n",
    "        \n",
    "        if checks_since_last_improvement > max_checks_without_improvement:\n",
    "            print(\"Early stopping!\")\n",
    "            saver.restore(sess, \"./my_best_model_so_far\")\n",
    "            break\n",
    "            \n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Final accuracy\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_final_mnist_early_stop_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation:\n",
    "\n",
    "This technique consists of generating new training instances from existing one, artificially boosting the size of the training set. \n",
    "\n",
    "Tensorflow offers several image manipulation operations. Check the API documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRACTICAL GUIDELINES!!!!\n",
    "\n",
    "These are *very* general guidelines. Your configuration may vary depending on the data. \n",
    "\n",
    "*Default configuration*:\n",
    "\n",
    "| Variable      | Configuration   |\n",
    "|---------------|-----------------|\n",
    "| Initialization| He              |\n",
    "| Activation    | ELU             |\n",
    "| Normalization | Batch Norm      |\n",
    "| Regularization| Dropout         |\n",
    "| Optimizer     | Nesterov        |\n",
    "| LR Schedule   | None            |\n",
    "\n",
    "*How to tweak things*:\n",
    "- Can't find a good learning rate, add a lr schedule.\n",
    "- Need sparse model, run L1 regularization. \n",
    "- Training set too small implement data augmentation\n",
    "- Need faster model at runtime, drop batch norm, replece ELU with leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. https://www.tensorflow.org/guide/\n",
    "2. Geron, Aurelion. *Hands On Machine Learning With Sci-Kit Learn and Tensorflow*. O'Reilly, 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
