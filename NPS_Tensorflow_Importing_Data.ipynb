{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing data\n",
    "`tf.data.Dataset` for an imaging pipeline represents one or more tensor objects. For imaging this might be a simple training example with pair of tensors representing the image data and label. \n",
    "  - creating a source (Dataset.from_tensor_slices()) constructs a dataset from one or more `tf.Tensor` objects. \n",
    "  - apply a transformation with one or more `tf.data.Dataset` objects\n",
    "  \n",
    " `tf.data.Iterator` provides the main way to extract elements from a dataset. The oipeeation returned by `Iterator.get_next()` yields the next element of a Dataset when executed. This will act as an interface between input pipeline code and your model. m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.TFRecordDataset` is good if you have your input data written to disk somewhere, which you can construct a `tf.data.TFRecordDataset`. Once you have the `Dataset` object you can transform it into a new `Dataset` by chaining methods on `tf.data.Dataset` object. \n",
    "    \n",
    "Most common way to consume values from a `Dataset` is to make a `iterator` object that provides access to one element of the dataset at a time. A `tf.data.Iterator` provides two operations: `Iterator.initializer`, which enables you to reinitialize the iterator's state and `Iterator.get_next()`, which returns `tf.Tensor` objects that corresponds to the next symbolic element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Structure \n",
    "An element contains one or more tf.Tensor objects, called components. Each component has a `tf.DType` representing the type of elements in the tensor, and a `tf.TensorShape` representing the (possibly partially specified) static shape of each element. The `Dataset.output_types` and `Dataset.output_shapes` properties allow you to inspect the inferred types and shapes of each component of a dataset element. The nested structure of these properties map to the structure of an element, which may be a single tensor, a tuple of tensors, or a nested tuple of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)\n",
    "print(dataset1.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random_uniform([4]),\n",
    "     tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)\n",
    "print(dataset2.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)\n",
    "print(dataset3.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(5)\n",
    "y = np.arange(5)\n",
    "print(x)\n",
    "print(y)\n",
    "z = zip(x,y)\n",
    "list(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the iterator\n",
    "`tf.data` supports the following iterators: \n",
    "##### one-shot: \n",
    "is the simplest form of iterator, supports iterating once through dataset with no need for explicit initialization. These will handle almost all of the cases that exist queue-based input pipeline but do not support parameterization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### intializable:\n",
    "\n",
    "you must run an explicit `iterator.initializer` op before using. in exchange you can parameterize the definition of the dataset using one or more `tf.placeholder()` tensors that can be fed when you initialize the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize an iterator over a dataset with 10 elements.\n",
    "    sess.run(iterator.initializer, feed_dict={max_value:10})\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value:100})\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reintializable:\n",
    "\n",
    "can be from multiple different `Dataset` objects. For example, you have a training input pipeline that uses random perturbations to the input images to improve generalization, and a validation input pipeline that evals predictions on unmodified data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure\n",
    "training_dataset = tf.data.Dataset.range(100).map(lambda x: x + tf.random_uniform([],-10, 10, tf.int64))\n",
    "validation_dataset = tf.data.Dataset.range(50)\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run 20 epochs in which the training dataset is traversed, followed by validation dataset\n",
    "    for _ in range(20):\n",
    "        # Initialize an iterator over the training dataset\n",
    "        sess.run(training_init_op)\n",
    "        for _ in range(100):\n",
    "            sess.run(next_element)\n",
    "        \n",
    "        # Initialize an iterator over the validation dataset\n",
    "        sess.run(validation_init_op)\n",
    "        for _ in range(50):\n",
    "            sess.run(next_element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feedable:\n",
    "\n",
    "can be used with `tf.placeholder` to select what `Iterator` to use in each call to `tf.Session.run` via the `feed_dict` mechanism. It offers the same functionality as a reintializable iterator but does not require to initialize the iterator from the start of dataset when you switch between iterators. We can use the same training and validation example from above using `tf.data.Iteraotr.from_string_handle` to define a feedable iterator that allows you to swtich between two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(lambda x: x + tf.random_uniform([],-10, 10, tf.int64)).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feedable iterator is defined by a handle placeholder and its structure. \n",
    "# We could us the output_types and output_shapes properties of with the \n",
    "# training_dataset or validation_dataset because they have identical structure\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use the feedable iterators with a variety of different kinds of iterator (like \n",
    "# one-shot and initializable iterators)\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # the 'Iterator.string_handle()' method returns a tensor that can be evaluated and used to feed\n",
    "    # the 'handle' placeholder\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    \n",
    "    # loop forever, alternating between training and validatioin\n",
    "    while True:\n",
    "        # Run 200 steps using the training dataset. Note, that dataset is infinite and we\n",
    "        # resume where  we left off in the previous 'while' loop\n",
    "        # iteration. \n",
    "        for _ in range(200):\n",
    "            sess.run(next_element, feed_dict={handle: training_handle})\n",
    "        \n",
    "        # Run onw pass over the validation dataset.\n",
    "        for _ in range(50):\n",
    "            sess.run(next_element, feed_dict={handle: validation_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONSUMING VALUES FROM AN ITERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Iterator.get_next()` method returns one or more `tf.Tensor` objects that correspond to the symbolic next element of an iterator. Each time these tensors are evaluated tehy take the value of the next element in the underlying dataset. Note, calling `Iterator.get_next()` does not immediately advance the iterator. You must have returned `tf.Tensor` object in an expression and pass the result to a `tf.Session.run()` to get the next elements. \n",
    "\n",
    "Note, that the `Iterator.get_next()` will raise a `tf.errors.OutofRangeError` once it reached the end of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(50)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically the 'result' will be the output of a model or optimizer's training op\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    print(sess.run(result))\n",
    "    print(sess.run(result))\n",
    "    print(sess.run(result))\n",
    "    print(sess.run(result))\n",
    "    print(sess.run(result))\n",
    "    try:\n",
    "        sess.run(result)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a common pattern for wrapping the \"training loop\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(result)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if each element fo the dataset has a nested structure the return value of Iterator.get_next()\n",
    "# will be one or more tf.Tensor in same structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comsuming numpy arrays\n",
    "# use np.load() to load training data into two arrays\n",
    "# features and labels will be embedded into your tensorflow graph as tf.constant() ops\n",
    "# this is good for small datasets but will waste memory and can run into the 2GB limit for\n",
    "# tf.GraphDef protocol buffer\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "    \n",
    "    assert features.shape[0] == labels.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comsuming numpy arrays\n",
    "# use np.load() to load training data into two arrays\n",
    "# features and labels will be embedded into your tensorflow graph as tf.constant() ops\n",
    "# this is good for small datasets but will waste memory and can run into the 2GB limit for\n",
    "# tf.GraphDef protocol buffer\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "    \n",
    "    assert features.shape[0] == labels.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively we can define the Dataset in terms of tf.placeholder() tensors and \n",
    "# feed the NumPy arrays when you initialize the Iterator over dataset\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "    \n",
    "# Assume taht each row of feattures correspnds to the same row as labels. \n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, features.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "# [Other transformations on 'dataset']\n",
    "dataset = ...\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFRecord data format\n",
    " Used to process large amounts of data that does not fit in memory. `tf.data.TFRecordDataset` class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataset that reads all of the examples from two files. \n",
    "filenames = [\"/var/data/file1.tf.record\", \"/var/data/file2.tf.record\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # parse the record into tensors. \n",
    "dataset = dataset.repeat()  # repeat the input indefinitely.\n",
    "dataset = dataset.batch(32) \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# you can feed the initializer with the app filenames for the current\n",
    "# phase of execution \n",
    "\n",
    "# initilize 'iterator' with training data\n",
    "training_filenames = [\"/var/data/file1.tfrecord\", \"var/data/file2.tfrecord\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# initialize 'iterator' with validation data\n",
    "training_filenames = [\"/var/data/file1.tfrecord\", \"var/data/file2.tfrecord\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consuming text data: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Consuming CSV data: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data with `Dataset.map()` \n",
    "\n",
    "`Dataset.map(f)` produces a new dataset by applying a given function `f` to each element of the input dataset. It's based on `map() function` in Python. `f` takes the `tf.Tensor` objects that reperent a single element in the input and returns a `tf.Tensor` object that represents ta single element in the new dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parsing `tf.Example` protocol buffer messages\n",
    "\n",
    "Input pipelines extract `tf.train.Example` protocol buffer messages from TFRecord-format file using `tf.python_io.TFRecordWriter`). Each `tf.train.Example` contains one or more \"features\" and input pipeline converts these into tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms scalar string 'example_proto' into a pair of scalar string and \n",
    "# a scalar integer, represents a image and it's label\n",
    "def _parse_fucntion(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "                \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"image\"], parsed_feature[\"label\"]\n",
    "\n",
    "# Creates a dataset that reads all of the examples from two files, and extracts\n",
    "# the image and label feeatures\n",
    "filenames =[\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoding image data and resizing it\n",
    "\n",
    "When we want to resize an image to a common size, so that they might be batched into a fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a image from a file, decodes it into a dense tensor, and resizes it \n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decoded_jpeg(image_string)\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "# A vector of filenames\n",
    "filenames = tf.constant([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\"])\n",
    "\n",
    "# labels[i] is the label for the image in 'filenames[i]'\n",
    "labels = tf.constant([0, 37, ...])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply arbitrary Python logic with tf.py_func()\n",
    "# Should use TF operations to preprocess data.\n",
    "# Sometimes it's helpful to use external libraries when passing in data.\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Use a custom OpenCv function to read an image, insted of the standard\n",
    "# TF 'tf.read_file()' operation\n",
    "\n",
    "def _read_py_function(filename, label):\n",
    "    image_decoded = cv2.imread.(filename.decode(), cv2.IMREAD_GRAYSCALE)\n",
    "    return image_decoded, label\n",
    "\n",
    "# Use standardized TF operation to resize the image to a fixed shape. \n",
    "def _resize_function(image_decoded, label):\n",
    "    image_decoded.set_shape([None, None, None])\n",
    "    image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
    "    return image_resized, label\n",
    "\n",
    "filenames = [\"/var/data/image1.jpg\", \"/var/data/image2.jpg\", ...]\n",
    "labels = [0, 37, 29, 1, ...]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(\n",
    "    lambda filename, label: tuple(tf.py_func(\n",
    "        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\n",
    "dataset = dataset.map(_resize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching dataset elements\n",
    "\n",
    "##### Simple batching\n",
    "Simplest is to form batch stacks n consecutive elements into a single element. The `Dataset.batch()` transform does this with same constraints as `tf.stack()` operator applied to each componenet of the elements i.e. for each component i, all elements mush have a tensor of the exact same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 0, -1, -2, -3]))\n",
      "(array([4, 5, 6, 7]), array([-4, -5, -6, -7]))\n",
      "(array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11]))\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "# what if we have tensors of different sizes (sequence models)\n",
    "# use Dataset.passed_batch() transofmr to enable to batch tensors of different shape tp \n",
    "# specify padding\n",
    "\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes = (None,))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training workflows\n",
    "\n",
    "##### Multiple epochs\n",
    "\n",
    "We can use tf.data in 2 ways to process multiple epochs of the same data. \n",
    "\n",
    "To create a dataset that repeats its input for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.repeat(10)  # this will repeat the input 10 times\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a training loop to receieve a signal at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Compute for 100 epochs.\n",
    "for _ in range(100):\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(next_element)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "  # [Perform end-of-epoch calculations here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomly shuffling input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.shuffle() transform randomly shuffles the input dataset, it maintains a fixed-size \n",
    "# buffer and chooses the next element uniformly at random from the buffer\n",
    "\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
